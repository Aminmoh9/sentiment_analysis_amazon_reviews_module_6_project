"""
TRUE Full Dataset Processing Pipeline
Processes the original JSONL files (2.1M reviews, 94K products)
Creates properly representative samples from the complete dataset
"""
import pandas as pd
import json
import ast
from datetime import datetime
import numpy as np

def load_jsonl_efficiently(file_path, max_records=None):
    """Load JSONL file efficiently with optional record limit"""
    records = []
    with open(file_path, 'r') as f:
        for i, line in enumerate(f):
            if max_records and i >= max_records:
                break
            try:
                records.append(json.loads(line))
            except json.JSONDecodeError:
                continue
    return pd.DataFrame(records)

def extract_image_url(images_data):
    """Extract the first valid image URL from images data"""
    # Handle None, NaN, or empty values
    if images_data is None or (isinstance(images_data, float) and pd.isna(images_data)):
        return None
    
    # Handle numpy arrays or pandas series
    if hasattr(images_data, '__len__') and not isinstance(images_data, (str, dict, list)):
        if len(images_data) == 0:
            return None
        images_data = images_data[0] if len(images_data) > 0 else None
    
    if not images_data:
        return None
    
    try:
        # Handle different image data formats
        if isinstance(images_data, str):
            if images_data.startswith('http'):
                return images_data
            elif images_data.startswith('['):
                images = ast.literal_eval(images_data)
            else:
                return None
        elif isinstance(images_data, list):
            images = images_data
        else:
            return None
            
        if isinstance(images, list) and len(images) > 0:
            first_image = images[0]
            if isinstance(first_image, dict):
                # Try different image size keys
                for key in ['large', 'hi_res', 'thumb']:
                    if key in first_image and first_image[key]:
                        url = first_image[key]
                        if isinstance(url, str) and url.startswith(('http://', 'https://')):
                            return url
            elif isinstance(first_image, str) and first_image.startswith(('http://', 'https://')):
                return first_image
    except Exception:
        pass
    
    return None

def clean_text_field(text):
    """Clean text fields"""
    if pd.isna(text) or not str(text).strip() or str(text).lower() in ['nan', 'none', 'null', '']:
        return ""
    return str(text).strip()

def validate_rating(rating):
    """Ensure rating is valid (1-5)"""
    try:
        rating_val = float(rating)
        if 1 <= rating_val <= 5:
            return rating_val
    except (ValueError, TypeError):
        pass
    return None

def extract_price(price_val):
    """Extract clean price value"""
    if pd.isna(price_val) or str(price_val).strip() == '':
        return None
    
    try:
        price_str = str(price_val).replace(',', '').replace('$', '').strip()
        if price_str == '' or price_str == 'nan':
            return None
        price_float = float(price_str)
        if 0.01 <= price_float <= 100000:
            return f"${price_float:.2f}"
    except (ValueError, TypeError):
        pass
    
    return None

def assign_sentiment(rating):
    """Assign sentiment based on rating"""
    if pd.isna(rating):
        return "Unknown"
    
    rating = float(rating)
    if rating >= 4:
        return "Positive"
    elif rating >= 3:
        return "Neutral"
    else:
        return "Negative"

def add_time_features(df, date_column='timestamp'):
    """Add time-based features from timestamp"""
    df = df.copy()
    
    # Convert timestamp to datetime (timestamps are in milliseconds)
    try:
        # First check if timestamp exists and is not empty
        if date_column not in df.columns:
            print(f"   Warning: {date_column} column not found, using current date")
            df['review_date'] = datetime.now()
        else:
            df[date_column] = pd.to_numeric(df[date_column], errors='coerce')
            # Check if we have valid timestamps
            valid_timestamps = df[date_column].notna()
            if valid_timestamps.sum() == 0:
                print(f"   Warning: No valid timestamps found, using current date")
                df['review_date'] = datetime.now()
            else:
                # Convert from milliseconds to seconds for pd.to_datetime
                df['review_date'] = pd.to_datetime(df[date_column] / 1000, unit='s', errors='coerce')
                # Fill NaT values with current date
                df['review_date'] = df['review_date'].fillna(datetime.now())
    except Exception as e:
        print(f"   Warning: Date conversion failed ({e}), using current date")
        df['review_date'] = datetime.now()
    
    # Add time features
    df['year'] = df['review_date'].dt.year
    df['month'] = df['review_date'].dt.month
    df['quarter'] = df['review_date'].dt.quarter
    df['weekday'] = df['review_date'].dt.dayofweek
    df['is_weekend'] = df['weekday'].isin([5, 6])
    df['review_age_days'] = (datetime.now() - df['review_date']).dt.days
    
    return df

def create_stratified_sample(df, sample_size, random_state=42):
    """Create stratified sample maintaining data distribution"""
    
    if len(df) <= sample_size:
        return df
    
    try:
        # Stratify by main_category and rating
        samples = []
        
        # Get proportions
        category_props = df['main_category'].value_counts(normalize=True)
        
        for category in category_props.index:
            category_df = df[df['main_category'] == category]
            category_sample_size = max(1, int(sample_size * category_props[category]))
            
            if len(category_df) > category_sample_size:
                # Further stratify by rating within category
                rating_groups = category_df.groupby('rating')
                category_sample = pd.DataFrame()
                
                for rating, group in rating_groups:
                    group_size = max(1, int(category_sample_size * len(group) / len(category_df)))
                    group_sample = group.sample(n=min(group_size, len(group)), random_state=random_state)
                    category_sample = pd.concat([category_sample, group_sample])
                
                # If we need more, randomly sample the remainder
                if len(category_sample) < category_sample_size:
                    remaining_needed = category_sample_size - len(category_sample)
                    remaining_df = category_df.drop(category_sample.index)
                    if len(remaining_df) > 0:
                        additional = remaining_df.sample(n=min(remaining_needed, len(remaining_df)), random_state=random_state)
                        category_sample = pd.concat([category_sample, additional])
            else:
                category_sample = category_df
            
            samples.append(category_sample)
        
        result = pd.concat(samples, ignore_index=True)
        
        # Final adjustment if needed
        if len(result) > sample_size:
            result = result.sample(n=sample_size, random_state=random_state)
        
        return result
        
    except Exception as e:
        print(f"Stratified sampling failed: {e}, using random sampling")
        return df.sample(n=sample_size, random_state=random_state)

def process_true_full_dataset():
    """Process the actual full JSONL dataset"""
    
    print("üöÄ Processing TRUE Full Amazon Appliances Dataset")
    print("=" * 60)
    print("üìä Source: 2.1M+ reviews, 94K+ products from JSONL files")
    
    # Step 1: Load full datasets (with memory management)
    print("\nüìÅ Step 1: Loading full JSONL datasets...")
    
    try:
        # Load reviews in chunks for memory efficiency
        print("   Loading reviews (this may take a few minutes)...")
        reviews_df = load_jsonl_efficiently("data/Appliances.jsonl", max_records=100000)  # 100K for better sample
        print(f"   ‚úÖ Reviews loaded: {len(reviews_df):,} records")
        
        # Load metadata  
        print("   Loading product metadata...")
        meta_df = load_jsonl_efficiently("data/meta_Appliances.jsonl")
        print(f"   ‚úÖ Metadata loaded: {len(meta_df):,} records")
        
    except Exception as e:
        print(f"   ‚ùå Error loading JSONL files: {e}")
        return None, None, None
    
    # Step 2: Clean reviews data
    print("\nüßπ Step 2: Cleaning reviews data...")
    reviews_clean = reviews_df.copy()
    original_count = len(reviews_clean)
    
    # Validate ratings
    reviews_clean['rating'] = reviews_clean['rating'].apply(validate_rating)
    reviews_clean = reviews_clean.dropna(subset=['rating'])
    print(f"   ‚úÖ Valid ratings: {len(reviews_clean):,} / {original_count:,}")
    
    # Clean text fields
    reviews_clean['title'] = reviews_clean['title'].apply(clean_text_field)
    reviews_clean['text'] = reviews_clean['text'].apply(clean_text_field)
    
    # Remove empty reviews
    reviews_clean = reviews_clean[
        (reviews_clean['title'] != "") | (reviews_clean['text'] != "")
    ]
    print(f"   ‚úÖ With content: {len(reviews_clean):,}")
    
    # Add sentiment and clean numeric fields
    reviews_clean['sentiment'] = reviews_clean['rating'].apply(assign_sentiment)
    reviews_clean['helpful_vote'] = pd.to_numeric(reviews_clean['helpful_vote'], errors='coerce').fillna(0)
    
    # Add time features
    reviews_clean = add_time_features(reviews_clean, 'timestamp')
    print(f"   ‚úÖ With valid dates: {len(reviews_clean):,}")
    
    # Step 3: Clean metadata
    print("\nüßπ Step 3: Cleaning product metadata...")
    meta_clean = meta_df.copy()
    original_meta_count = len(meta_clean)
    
    # Clean titles and categories
    meta_clean['title'] = meta_clean['title'].apply(clean_text_field)
    meta_clean = meta_clean[meta_clean['title'] != ""]
    print(f"   ‚úÖ With titles: {len(meta_clean):,} / {original_meta_count:,}")
    
    meta_clean['main_category'] = meta_clean['main_category'].apply(clean_text_field)
    meta_clean = meta_clean[meta_clean['main_category'] != ""]
    print(f"   ‚úÖ With categories: {len(meta_clean):,}")
    
    # Extract images and prices
    meta_clean['main_image'] = meta_clean['images'].apply(extract_image_url)
    images_count = meta_clean['main_image'].notna().sum()
    print(f"   ‚úÖ With valid images: {images_count:,}")
    
    meta_clean['price_clean'] = meta_clean['price'].apply(extract_price)
    price_count = meta_clean['price_clean'].notna().sum()
    print(f"   ‚úÖ With valid prices: {price_count:,}")
    
    # Step 4: Merge datasets
    print("\nüîó Step 4: Merging cleaned datasets...")
    
    merged_df = pd.merge(
        reviews_clean,
        meta_clean,
        left_on="asin",
        right_on="parent_asin",
        how="inner",
        suffixes=('_review', '_product')
    )
    
    print(f"   ‚úÖ Complete merged records: {len(merged_df):,}")
    
    # Create final structure
    final_df = pd.DataFrame({
        # Review information
        'asin': merged_df['asin'],
        'rating': merged_df['rating'],
        'review_title': merged_df['title_review'],
        'review_text': merged_df['text'],
        'review_date': merged_df['review_date'],
        'helpful_vote': merged_df['helpful_vote'],
        'sentiment': merged_df['sentiment'],
        'user_id': merged_df['user_id'],
        'verified_purchase': merged_df.get('verified_purchase', True),
        
        # Time features
        'year': merged_df['year'],
        'month': merged_df['month'],
        'quarter': merged_df['quarter'],
        'weekday': merged_df['weekday'],
        'is_weekend': merged_df['is_weekend'],
        'review_age_days': merged_df['review_age_days'],
        
        # Product information
        'product_title': merged_df['title_product'],
        'main_category': merged_df['main_category'],
        'main_image': merged_df['main_image'],
        'price': merged_df['price_clean'],
        'average_rating': pd.to_numeric(merged_df.get('average_rating', 0), errors='coerce'),
        'rating_number': pd.to_numeric(merged_df.get('rating_number', 0), errors='coerce'),
        'parent_asin': merged_df['parent_asin_product']
    })
    
    # Step 5: Create representative samples from TRUE full data
    print("\nüìä Step 5: Creating representative samples from full dataset...")
    
    # 50-record stratified sample
    sample_50 = create_stratified_sample(final_df, 50)
    print(f"   ‚úÖ 50-record sample: {sample_50['main_category'].nunique()} categories, {sample_50['year'].min()}-{sample_50['year'].max()}")
    
    # 10K-record stratified sample
    sample_10k = create_stratified_sample(final_df, min(10000, len(final_df)))
    print(f"   ‚úÖ 10k-record sample: {sample_10k['main_category'].nunique()} categories, {sample_10k['year'].min()}-{sample_10k['year'].max()}")
    
    # Step 6: Save datasets
    print("\nüíæ Step 6: Saving clean datasets...")
    
    # Save full dataset (for reference, not loaded in app)
    final_df.to_csv("data/appliances_clean.csv", index=False)
    
    # Save working samples (used by app)
    sample_50.to_csv("data/appliances_50_sample.csv", index=False)
    sample_10k.to_csv("data/appliances_10k_sample.csv", index=False)
    
    print(f"   ‚úÖ Saved appliances_clean.csv ({len(final_df):,} records) - Reference dataset")
    print(f"   ‚úÖ Saved appliances_50_sample.csv (50 records) - App sample")
    print(f"   ‚úÖ Saved appliances_10k_sample.csv ({len(sample_10k):,} records) - App sample")
    
    # Step 7: Final report
    print(f"\nüéâ DATASET PROCESSING COMPLETE!")
    print("=" * 60)
    print(f"üìä **Results from 2.1M+ Reviews ‚Üí {len(final_df):,} Clean Records:**")
    print(f"   ‚Ä¢ Date range: {final_df['review_date'].min().strftime('%Y-%m-%d')} to {final_df['review_date'].max().strftime('%Y-%m-%d')}")
    print(f"   ‚Ä¢ Categories: {final_df['main_category'].nunique()}")
    print(f"   ‚Ä¢ 100% complete metadata")
    
    print(f"\nüéØ **App Usage:**")
    print(f"   ‚Ä¢ Use appliances_50_sample.csv for quick preview")
    print(f"   ‚Ä¢ Use appliances_10k_sample.csv for full analytics")  
    print(f"   ‚Ä¢ appliances_clean.csv kept as reference (not loaded in app)")
    
    print(f"\nüöÄ **Ready to run: streamlit run app.py**")
    
    return final_df, sample_50, sample_10k

if __name__ == "__main__":
    full_df, sample_50, sample_10k = process_true_full_dataset()